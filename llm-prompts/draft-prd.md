# Draft / Review PRDs

---

## **1. Purpose**
These instructions enable LLMs to help users create clear, outcome-focused Product Requirements Documents (PRDs) for product and platform teams that drive alignment, informed decision-making, and successful product launches.

While optimized for product/platform work at Cloudflare (DevTools, Data Platform, Developer Infrastructure, etc.), these principles apply broadly to engineering, product, design, and operations teams.

LLMs must support the user by:
- enforcing proper PRD structure and completeness,
- maintaining outcome > output discipline,
- producing concise and unambiguous language,
- ensuring user/customer problems are well-understood before jumping to solutions,
- grounding requirements in measurable success criteria,
- and helping teams make informed decisions through structured thinking.

---

# **2. Core Principles of High-Quality PRDs**

## **2.1 Problem-First Thinking**
**Problems** describe real user/customer pain, inefficiency, or unmet needs.
**Solutions** describe how you plan to address those problems.

LLMs must ensure PRDs clearly articulate the problem before jumping to solutions.

### **Examples**
| Good Problem Statement (Correct) | Bad Solution-First Statement (Incorrect) |
|----------------------------------|------------------------------------------|
| "Engineers waste 2-3 hours per week searching for internal documentation across fragmented systems." | "We need to build a new developer portal." |
| "Data pipeline failures go undetected for hours, causing downstream quality issues." | "We should implement better monitoring." |
| "New developers take 2+ weeks to understand our build system due to lack of clear documentation." | "We need to create onboarding docs." |

---

## **2.2 Clarity, Brevity, Precision**
- Avoid jargon, filler, or marketing speak
- Prefer simple, direct sentences
- Focus on what matters to users, teams, and the business
- Use active voice and present tense where possible
- Be specific about metrics, timelines, and success criteria

---

## **2.3 Customer-Centric Requirements**
Requirements should be grounded in real user needs, not assumptions.

- Reference user research, data, or feedback where available
- Distinguish between "must have" and "nice to have" features
- Explain the user value for each requirement
- Avoid building features that don't solve real problems

---

## **2.4 Measurable Success Criteria**
Every PRD must clearly define how success will be measured.

- Use specific, quantifiable metrics when possible
- Include baseline measurements and targets
- Explain how metrics will be collected and tracked
- Connect metrics to business outcomes or user value

---

## **2.5 Iterative Delivery Over Big Bang**
PRDs should plan for incremental releases that deliver value early and enable learning.

- Break work into small, shippable increments
- Define clear release milestones
- Plan for feedback loops and iteration
- Avoid waterfall-style "launch everything at once" approaches

---

# **3. Common Anti-Patterns LLMs Must Avoid**

LLMs must actively guard against these frequent mistakes:

1. **Jumping to solutions before understanding the problem** - Always start with user pain, not features
2. **Vague or unmeasurable success criteria** - "Improve developer experience" is not a success metric
3. **Missing target audience definition** - Who specifically is this for?
4. **Listing features without explaining user value** - Every requirement needs a "why"
5. **Ignoring alternatives and competitive landscape** - Understand what exists before building
6. **No clear go-to-market or adoption plan** - How will users discover and adopt this?
7. **Skipping dependencies and risks** - What could go wrong? What do we need from others?
8. **Overly detailed implementation specs** - PRDs define the problem and requirements, not the exact implementation
9. **No clear release strategy** - How will this be broken down and shipped incrementally?
10. **Missing non-goals** - What are we explicitly NOT doing?

When reviewing PRDs, LLMs should scan for these patterns and push back immediately.

---

# **4. Definitions and Examples of Every PRD Component**

Below are detailed definitions for each section of a PRD, with examples of what good looks like.

---

## **4.1 Product Opportunity Assessment**

This section, based on Marty Cagan's framework, ensures teams think through the fundamental questions before diving into solutions.

---

### **4.1.1 What opportunity (problem, need, or desire) will this project address?**

**Definition:**
A clear description of the user/customer problem, need, or desire that motivates this work. This is the value proposition.

**Characteristics:**
- Describes a real pain point or unmet need
- Grounded in evidence (user feedback, data, research)
- Avoids solution language
- Explains the impact of NOT solving this problem

#### Good Examples
- "Internal engineers spend 2-3 hours per week searching for documentation across 5+ fragmented systems (Confluence, GitHub wikis, Google Docs, Notion, internal wikis). This slows onboarding (2+ weeks to productivity) and increases duplicate work as teams can't find existing solutions."

- "Data pipeline failures currently go undetected for 2-4 hours on average, causing downstream data quality issues that affect business reporting and analytics. This erodes trust in data products and requires costly manual remediation."

- "Developers deploying new services must manually configure 15+ infrastructure settings, taking 4-6 hours and frequently resulting in production incidents due to misconfiguration. This slows time-to-market and increases operational burden."

#### Bad Examples
- "We need better documentation." (This is a solution, not a problem)
- "Our developer experience could be improved." (Too vague, not grounded in evidence)
- "We should modernize our infrastructure." (Solution-first thinking)

---

### **4.1.2 For whom do we address that opportunity?**

**Definition:**
A specific description of the target user(s) or customer segment(s).

**Characteristics:**
- Specific and concrete (not "all developers")
- May include personas or user segments
- Explains relevant context about the target audience
- Distinguishes primary from secondary users if applicable

#### Good Examples
- "**Primary:** Backend engineers at Cloudflare deploying new services (200+ engineers across 15+ teams). **Secondary:** SREs who support these services in production."

- "**Primary:** Data engineers building and maintaining data pipelines (30+ engineers across Data Platform, Analytics, and ML teams). **Secondary:** Data analysts and scientists who consume pipeline outputs."

- "**Primary:** New engineers in their first 90 days at Cloudflare (50-75 new hires per quarter). **Secondary:** Engineering managers and onboarding buddies who support new hires."

#### Bad Examples
- "Developers" (Too broad and vague)
- "Anyone who uses our platform" (Not specific enough to drive design decisions)
- "Internal teams" (Which teams? What roles?)

---

### **4.1.3 How will we measure success/make money from this product?**

**Definition:**
Specific, quantifiable metrics that define success, plus revenue strategy if applicable.

**Characteristics:**
- Concrete, measurable metrics with baselines and targets
- Explains how metrics will be collected
- Connects to business value or user outcomes
- Includes timeframe for achieving targets
- For internal tools: focus on efficiency, velocity, quality, or satisfaction metrics

#### Good Examples
- "**Primary metric:** Reduce mean time to first successful deployment for new engineers from 2 weeks to â‰¤3 days, measured via deployment telemetry. Target: achieve by end of Q2 2025.
  **Secondary metrics:** New hire satisfaction score (NPS) increases from 6.5 to â‰¥8.0 in 90-day surveys; 80% of new hires report feeling productive within first week."

- "**Primary metric:** Reduce data pipeline MTTR from 4 hours to <30 minutes, measured via incident tracking. Target: achieve within 6 months.
  **Secondary metrics:** Data quality incident rate decreases by â‰¥40%; data consumer trust score (quarterly survey) increases from 6.2 to â‰¥8.0."

- "**Revenue strategy:** This feature enables us to launch a new "Enterprise Developer Tools" tier at $50/seat/month. Based on customer research with 15 enterprise customers, we project 30% adoption among enterprise accounts within 12 months, generating ~$2M ARR."

#### Bad Examples
- "Improve developer experience" (Not measurable)
- "Increase adoption" (No baseline, target, or timeframe)
- "Make customers happy" (Too vague)
- "We'll track some metrics" (No specific metrics defined)

---

### **4.1.4 What alternatives are out there?**

**Definition:**
An analysis of competitive or alternative solutions, including internal alternatives like "do nothing."

**Characteristics:**
- Covers both external products and internal alternatives
- Explains strengths and weaknesses of alternatives
- Articulates why existing solutions don't solve the problem
- Includes the "do nothing" or "manual workaround" alternative

#### Good Examples
- "**External alternatives:**
  - Backstage (Spotify): Open-source developer portal. Strong plugin ecosystem but requires significant customization and operational overhead.
  - GitHub Docs: Good for code documentation but doesn't integrate with our internal services or tooling.

  **Internal alternatives:**
  - Current state (fragmented docs): Teams use Confluence, GitHub wikis, Notion, and Google Docs. No unified search or discoverability.
  - Do nothing: Continue with status quo. Cost: ongoing productivity loss of 2-3 hours/week per engineer = ~$500K/year in lost productivity.

  **Why build:** Existing solutions don't integrate with our internal tooling ecosystem and don't solve our specific discoverability and consistency problems."

- "**External alternatives:**
  - Datadog: Comprehensive monitoring but expensive at our scale ($200K+/year) and doesn't integrate with our custom data infrastructure.
  - Prometheus + Grafana: Strong monitoring but requires manual setup and doesn't provide automated alerting for data quality issues.

  **Why build:** We need deep integration with our custom data pipeline infrastructure and automated data quality checks that external tools don't provide. ROI: avoiding $200K/year in licensing costs."

#### Bad Examples
- "There are some competitors but ours will be better." (Not specific)
- "Nothing else solves this problem." (Unlikely to be true; shows lack of research)
- "We looked at a few tools but didn't like them." (No analysis of why)

---

### **4.1.5 What factors are critical to success?**

**Definition:**
Specific, actionable requirements that are necessary for the product to succeed.

**Characteristics:**
- Prioritized (must-have vs. nice-to-have)
- Grounded in user needs
- Specific and testable
- Explains WHY each requirement matters
- Includes both functional and non-functional requirements

#### Good Examples - Structure

**Must-have requirements:**
1. **Unified search across all documentation sources** - Engineers must be able to search across Confluence, GitHub, and internal wikis from a single interface. Why: 80% of documentation friction comes from not knowing where to look.

2. **Sub-2 second search latency at P95** - Search results must return in <2 seconds for 95% of queries. Why: User research shows engineers abandon search after 3 seconds.

3. **Automatic content freshness indicators** - Docs must show last-updated timestamps and flag stale content (>6 months old). Why: 40% of current documentation is outdated, leading to wasted time and errors.

**Nice-to-have requirements:**
1. **AI-powered search suggestions** - Suggest related docs based on search context. Why: Could improve discoverability but not critical for v1.

2. **Inline code examples** - Embed runnable code snippets in documentation. Why: Improves developer experience but can be added post-launch.

**What will be different when shipped:**
- Engineers can find relevant documentation in <30 seconds (down from 15+ minutes currently)
- Documentation search becomes the default first step for problem-solving
- Documentation quality improves as teams see visibility metrics

**Non-goals:**
- We are NOT building a new documentation authoring system (teams continue using existing tools)
- We are NOT replacing GitHub as the source of truth for code documentation
- We are NOT building real-time collaboration features (out of scope for v1)

#### Bad Examples
- "Build a great search experience" (Not specific or testable)
- "Make it fast" (No quantitative target)
- "Include everything developers need" (Too vague)
- Long list of 20+ requirements with no prioritization

---

### **4.1.6 How will this product/feature be discovered and adopted?**

**Definition:**
The go-to-market strategy explaining how users will learn about and start using the product.

**Characteristics:**
- Specific distribution/communication channels
- Clear adoption milestones
- User onboarding plan
- Success criteria for adoption
- Identifies potential blockers to adoption

#### Good Examples
- "**Launch plan:**
  1. **Soft launch (Week 1-2):** Beta with DevTools team (15 engineers) to gather feedback and iterate
  2. **Pilot expansion (Week 3-6):** Expand to 3 additional engineering teams (50 engineers total) with structured feedback sessions
  3. **General availability (Week 7+):** Announce via Engineering All-Hands, email to all engineers, Slack #engineering channel

  **Onboarding:**
  - 5-minute interactive tutorial on first use
  - Weekly tips via Slack #dev-tips channel
  - Office hours (2x/week for first month) for questions

  **Adoption targets:**
  - Week 4: 50 active users (pilot teams)
  - Week 8: 200 active users (30% of engineering)
  - Week 16: 500+ active users (75% of engineering)

  **Measurement:** Track weekly active users, searches per user, and NPS via in-app survey."

- "**Distribution strategy:**
  - Integrate into existing developer workflows (VS Code extension, CLI tool, browser extension)
  - Make it the default search in internal developer homepage
  - Include in new hire onboarding checklist

  **Change management:**
  - Partner with engineering managers to demo in team meetings
  - Create comparison videos showing old vs. new workflow
  - Highlight time savings in weekly engineering newsletter"

#### Bad Examples
- "We'll announce it when it's ready." (No specific plan)
- "Build it and they will come." (No adoption strategy)
- "Send an email to everyone." (Not sufficient for behavior change)

---

## **4.2 Dependencies and Risks**

This section ensures teams think through potential obstacles and interdependencies before committing to work.

---

### **4.2.1 Known Risks**

**Definition:**
Potential issues, blockers, or negative consequences that could impact the project.

**Characteristics:**
- Specific and concrete
- Includes likelihood and impact assessment
- Identifies mitigation strategies
- Covers technical, organizational, and user adoption risks

#### Good Examples
- "**Risk: Low adoption due to workflow disruption**
  *Likelihood: Medium | Impact: High*
  Engineers may resist changing from familiar (though inefficient) search patterns.
  *Mitigation:* Integrate into existing workflows; make new tool strictly better (not just different); run pilots with influential teams first."

- "**Risk: Search quality doesn't meet user expectations**
  *Likelihood: Medium | Impact: High*
  If search relevance is poor, users will revert to old tools within weeks.
  *Mitigation:* Extensive beta testing with real queries; implement relevance feedback mechanism; set quality bar (95% of searches return relevant result in top 3)."

- "**Risk: Performance degradation at scale**
  *Likelihood: Low | Impact: Medium*
  Search latency may increase as index grows beyond 100K documents.
  *Mitigation:* Load testing with 5x projected scale; implement caching layer; performance monitoring with alerts."

#### Bad Examples
- "There might be some issues." (Too vague)
- "This could fail." (No specifics on likelihood, impact, or mitigation)
- "We'll figure it out as we go." (No proactive risk management)

---

### **4.2.2 Cross-Team Dependencies**

**Definition:**
Other teams or individuals whose work is required for this project to succeed.

**Characteristics:**
- Specific teams and individuals
- Clear description of what is needed
- Timeline requirements
- Status of coordination

#### Good Examples
- "**Infrastructure team:** Need Elasticsearch cluster provisioned and configured.
  *Timeline:* Required by Week 2 of development.
  *Status:* Initial conversation held; formal request submitted; Infrastructure PM confirmed availability."

- "**Security team:** Need security review for search indexing of private repositories.
  *Timeline:* Required before beta launch (Week 6).
  *Status:* Meeting scheduled for Week 2; preliminary approval received pending formal review."

- "**Documentation team:** Need partnership on content quality standards and freshness monitoring.
  *Timeline:* Ongoing collaboration throughout project.
  *Status:* Alignment meeting held; weekly sync established."

#### Bad Examples
- "We might need help from other teams." (Too vague)
- "Infrastructure team needs to do some stuff." (Not specific)
- "Dependencies TBD." (Should be identified before committing to work)

---

### **4.2.3 Customer Support Impact**

**Definition:**
Impact on customer support, including documentation, training, and support processes.

**Characteristics:**
- Specific documentation needs
- Training requirements
- Expected support volume
- Runbook or troubleshooting needs

#### Good Examples
- "**New documentation needed:**
  - User guide: "How to search across internal documentation" (~1 page)
  - FAQ covering common search scenarios
  - Video walkthrough (5 minutes)

  **Existing documentation updates:**
  - Update "Finding Documentation" section of New Hire Guide
  - Add to Developer Tools Overview page

  **Training:**
  - 30-minute demo at Engineering All-Hands
  - Self-service interactive tutorial (built into product)

  **Support plan:**
  - Create #dev-docs-search Slack channel for questions
  - Weekly office hours for first month
  - Expected support volume: High in first 2 weeks (50+ questions), stabilizing to <5 questions/week"

#### Bad Examples
- "We'll write some docs." (Not specific)
- "No support impact expected." (Unlikely for any new product)
- "Documentation TBD." (Should be planned upfront)

---

### **4.2.4 Analytics Impact**

**Definition:**
New instrumentation, metrics, or reporting needed to track product success.

**Characteristics:**
- Specific events and metrics to track
- Dashboard or reporting requirements
- Data retention and privacy considerations
- Timeline for instrumentation

#### Good Examples
- "**New instrumentation required:**
  - `search_query_submitted` event (properties: query_text_hash, user_id, timestamp, result_count)
  - `search_result_clicked` event (properties: query_id, result_rank, doc_id, user_id)
  - `doc_view` event (properties: doc_id, source, referrer, user_id, timestamp)

  **Metrics to track:**
  - Weekly active users (WAU)
  - Searches per user per week
  - Search success rate (clicks within top 3 results)
  - Time to first click
  - Repeat search rate (same query within 5 minutes = failed search)

  **Dashboard needs:**
  - Weekly metrics dashboard (WAU, searches, success rate)
  - Search quality dashboard (query analysis, zero-result queries, low-click queries)

  **Privacy:**
  - Query text will be one-way hashed (not stored in plaintext)
  - 90-day retention for event data

  **Timeline:**
  - Instrumentation implemented in Week 3
  - Dashboards ready by Week 5 (before beta launch)"

#### Bad Examples
- "We'll track usage." (Not specific)
- "Standard analytics will be fine." (May not capture product-specific metrics)
- "We can add tracking later." (Should be built in from the start)

---

### **4.2.5 QA Impact**

**Definition:**
Testing strategy including automated and manual testing needs.

**Characteristics:**
- Quality built in from the start
- Automated testing plan
- Manual testing scenarios if needed
- Test data requirements

#### Good Examples
- "**Automated testing:**
  - Unit tests for search indexing logic (target: 90% coverage)
  - Integration tests for search API (20+ test scenarios covering common and edge cases)
  - End-to-end tests for search UI (core user flows)
  - Performance tests: P95 latency <2s with 100 concurrent users

  **Manual testing:**
  - Beta user testing with 15 engineers (structured feedback sessions)
  - Accessibility testing (WCAG 2.1 AA compliance)
  - Cross-browser testing (Chrome, Firefox, Safari)

  **Test data:**
  - Create test documentation corpus (100 docs covering different sources and types)
  - Anonymized sample of real search queries from logs (for relevance testing)

  **Quality gates:**
  - All automated tests passing
  - P95 latency <2s in load tests
  - Zero critical security vulnerabilities
  - Beta user satisfaction â‰¥7.0/10"

#### Bad Examples
- "We'll test it before launching." (Not specific)
- "QA team will handle testing." (Quality should be built in, not inspected in)
- "We'll do some manual testing." (No clear test plan)

---

## **4.3 Solution Overview**

This section describes HOW you plan to address the opportunity, at a high level.

---

### **4.3.1 General Overview**

**Definition:**
A concise description of the proposed solution approach.

**Characteristics:**
- Describes the approach without over-specifying implementation
- Explains key technology or architecture decisions
- Includes diagrams or mockups where helpful
- Focuses on user-facing experience and value

#### Good Examples
- "**Solution approach:**
  Build a unified search interface that indexes documentation from Confluence, GitHub, and internal wikis into a centralized Elasticsearch cluster. Provide a clean, fast web UI with single search box that returns ranked results across all sources.

  **Key decisions:**
  - Use Elasticsearch for search (proven at scale, good relevance tuning)
  - Build custom indexing pipelines for each source (Confluence API, GitHub API, etc.)
  - Lightweight React frontend served from internal developer portal
  - Real-time indexing with 5-minute freshness guarantee

  **User experience:**
  - Single search box (similar to Google)
  - Instant results with source indicators (Confluence/GitHub/Wiki icons)
  - Filters: by source, by team, by date
  - Click result â†’ opens document in original source

  [Include mockup/wireframe here]"

- "**Solution approach:**
  Create automated data quality monitoring that continuously validates pipeline outputs against expected schemas, freshness, and completeness criteria. Alert on-call engineers within 5 minutes of detected issues.

  **Key components:**
  - Schema validation engine (comparing output to expected schema)
  - Freshness monitors (alerting if data hasn't updated in expected timeframe)
  - Completeness checks (row count validation, null rate monitoring)
  - Centralized alerting dashboard with PagerDuty integration

  **User experience:**
  - Data engineers define quality rules in YAML config
  - Automated validation runs after each pipeline execution
  - Alerts sent to on-call engineer with diagnostic context
  - Dashboard shows quality status for all pipelines"

#### Bad Examples
- "We'll build something to solve the problem." (Not specific)
- Extremely detailed technical implementation (should be in engineering design doc, not PRD)
- No explanation of user experience or key decisions

---

### **4.3.2 Breakdown of Proposed Tasks and Releases**

**Definition:**
A phased release plan showing incremental delivery of value.

**Characteristics:**
- Each release delivers standalone user value
- Releases are small and shippable (ideally â‰¤4 weeks each)
- Clear success criteria for each release
- Built-in learning and iteration points

#### Good Examples
- "**Release 1: MVP - Confluence Search (Weeks 1-4)**
  *Goal:* Prove search value with single source
  *Scope:*
  - Index all Confluence spaces
  - Basic web UI with search and results
  - P95 latency <2s
  *Success criteria:*
  - 50+ beta users searching weekly
  - Search success rate â‰¥70%
  - NPS â‰¥7.0

  **Release 2: GitHub Integration (Weeks 5-7)**
  *Goal:* Expand coverage to code documentation
  *Scope:*
  - Add GitHub wiki and README indexing
  - Source filtering in UI
  - Unified result ranking
  *Success criteria:*
  - 200+ active users
  - Search success rate â‰¥75%
  - GitHub results appear in â‰¥30% of searches

  **Release 3: Enhanced Discoverability (Weeks 8-10)**
  *Goal:* Improve search quality and add freshness indicators
  *Scope:*
  - Content freshness indicators
  - Related docs suggestions
  - Search quality dashboard for continuous improvement
  *Success criteria:*
  - 500+ active users (75% of engineering)
  - Search success rate â‰¥80%
  - Documentation update rate increases 20%"

- "**Release 1: Monitoring Foundation (Weeks 1-3)**
  *Scope:* Schema validation for 5 critical pipelines
  *Value:* Catch breaking schema changes immediately

  **Release 2: Freshness Monitoring (Weeks 4-5)**
  *Scope:* Automated freshness checks with alerting
  *Value:* Detect stale data within minutes

  **Release 3: Completeness Validation (Weeks 6-8)**
  *Scope:* Row count and null rate monitoring
  *Value:* Catch data quality degradation"

#### Bad Examples
- "We'll build everything and launch it all at once." (No iterative learning)
- Releases that don't deliver standalone user value
- Release timeline >12 weeks without user feedback
- Vague scope like "Implement features" with no details

---

### **4.3.3 Estimated Time Frame**

**Definition:**
Overall timeline from kickoff to general availability.

**Characteristics:**
- Realistic based on team capacity
- Includes buffer for unknowns
- Shows major milestones
- Acknowledges uncertainty where appropriate

#### Good Examples
- "**Overall timeline: 10 weeks from kickoff to GA**

  *Major milestones:*
  - Week 1-2: Infrastructure setup, indexing pipeline for Confluence
  - Week 3-4: MVP web UI, beta launch with DevTools team
  - Week 5-6: GitHub integration, expand to 3 pilot teams
  - Week 7-8: Enhanced features (freshness, filtering)
  - Week 9-10: GA launch, documentation, training

  *Assumptions:*
  - 2 engineers full-time
  - Infrastructure dependencies resolved within SLA
  - No major scope changes during development

  *Risks to timeline:*
  - Search relevance tuning may take longer than expected (could add 1-2 weeks)
  - Security review could identify issues requiring remediation"

- "**Overall timeline: 8 weeks**
  - Weeks 1-3: Release 1 (schema validation)
  - Weeks 4-5: Release 2 (freshness monitoring)
  - Weeks 6-8: Release 3 (completeness validation)

  *Note:* Timeline assumes no competing priorities for data platform team."

#### Bad Examples
- "It'll take as long as it takes." (Not helpful for planning)
- Overly precise estimates without acknowledging uncertainty ("Exactly 47 days")
- No milestones or breakdown
- Timeline with no assumptions or dependencies noted

---

# **5. PRD Document Structure and Formatting**

## **5.1 Required Format: Confluence Wiki Markup**

All PRD drafts MUST be written in Confluence Wiki Markup syntax, not Markdown.

### Key Syntax Rules:
- **Headers:** `h1.`, `h2.`, `h3.` (not `#`, `##`, `###`)
- **Bold:** `*bold text*` (not `**bold**`)
- **Italic:** `_italic text_` (not `*italic*`)
- **Tables:** `||` for headers, `|` for cells
- **Line breaks in tables:** `\\` (not `<br>`)
- **Horizontal rules:** `----` preceded by `\\`
- **Bullets and numbered lists:** NO extra line breaks between list items. List items (`*` for bullets, `#` for numbered) should be on consecutive lines with no blank lines between them.

See full Confluence Wiki Markup reference for complete syntax.

## **5.2 Required Document Structure**

LLMs must enforce the following structure unless explicitly instructed otherwise:

```
h1. PRD â€” [PROJECT NAME]

h1. ðŸš€ Product Opportunity Assessment

h2. What opportunity (problem, need, or desire) will this project address?

[Problem statement with evidence and impact]

h2. For whom do we address that opportunity?

[Target user(s) with specifics]

h2. How will we measure success/make money from this product?

[Specific metrics with baselines, targets, and measurement approach]

[For revenue-generating products: revenue strategy]

h2. What alternatives are out there?

[Analysis of external alternatives, internal alternatives, and "do nothing" option]

[Explanation of why building is the right choice]

h2. What factors are critical to success?

*Must-have requirements:*
# [Requirement 1 with why it matters]
# [Requirement 2 with why it matters]

*Nice-to-have requirements:*
# [Requirement 1 with why it matters]

*What will be different when shipped:*
* [Change 1]
* [Change 2]

*Non-goals:*
* [Explicitly NOT doing X]
* [Explicitly NOT doing Y]

h2. How will this product/feature be discovered and adopted?

[Go-to-market plan with distribution, onboarding, adoption targets, and measurement]

\\
----

h1. ðŸ˜¬ Dependencies and Risks

h2. Known risks

[Risk 1: Description | Likelihood | Impact | Mitigation]

[Risk 2: Description | Likelihood | Impact | Mitigation]

h2. Cross-team dependencies

[Team/person: What is needed | Timeline | Status]

h2. Customer Support impact

[Documentation needs, training, support plan, expected volume]

h2. Analytics impact

[New instrumentation, metrics, dashboards, privacy, timeline]

h2. QA impact

[Automated testing, manual testing, test data, quality gates]

\\
----

h1. ðŸ˜ƒ Solution Overview

h2. General overview of how we plan to address the opportunity

[Solution approach, key decisions, user experience]

[Include mockups/diagrams where helpful]

h2. Breakdown of proposed tasks and what goes into each release

*Release 1: [Name] (Timeline)*
* _Goal:_ [What user value does this deliver]
* _Scope:_ [What's included]
* _Success criteria:_ [How we know it worked]

*Release 2: [Name] (Timeline)*
* _Goal:_ [What user value does this deliver]
* _Scope:_ [What's included]
* _Success criteria:_ [How we know it worked]

h2. Estimated time frame

[Overall timeline with milestones, assumptions, and risks]
```

---

# **6. How LLMs Should Assist the User**

## **6.1 When drafting PRDs**
LLMs must:
- Start by understanding the problem before suggesting solutions
- Ask clarifying questions one at a time to gather necessary context
- Guide users through the Product Opportunity Assessment framework systematically
- Push for specific, measurable success criteria (avoid accepting vague metrics)
- Ensure target users are clearly defined
- Challenge solution ideas with "What problem does this solve?" and "How do we know users need this?"
- Help break large projects into incremental, shippable releases
- Maintain Confluence Wiki Markup formatting throughout

## **6.2 When reviewing PRDs**
LLMs should ask:
- "Is the problem clearly articulated with evidence?"
- "Are success metrics specific and measurable?"
- "Do we know who the target users are specifically?"
- "Have we considered alternatives?"
- "Are requirements prioritized (must-have vs. nice-to-have)?"
- "Is there a clear incremental release plan?"
- "Have dependencies and risks been identified?"
- "Is there a realistic go-to-market plan?"

## **6.3 When refining PRDs**
LLMs should steer the user back to:
- Problem-first thinking (not solution-first)
- Specificity and measurability
- User-centricity (not internal preferences)
- Incremental delivery (not big bang launches)
- Evidence and data (not assumptions)
- Clarity and brevity

## **6.4 Handling incomplete information**
When users don't have all the information needed:
- Ask focused, one-question-at-a-time clarifying questions
- Suggest what research or data gathering might help
- Explicitly mark sections as "[TBD - needs user research]" or similar
- Don't fabricate information or make unfounded assumptions
- Guide users to complete the most critical sections first (Problem, Target Users, Success Metrics)

---

# **7. Summary of Required LLM Behavior**

LLMs must:
- Guide users through problem-first thinking before jumping to solutions
- Push for specific, measurable success criteria
- Ensure target users are clearly and specifically defined
- Challenge vague requirements with questions like "How will we measure this?" and "Why does this matter to users?"
- Help break large projects into incremental, shippable releases
- Maintain Confluence Wiki Markup formatting consistently
- Ask clarifying questions one at a time when information is missing
- Flag anti-patterns immediately (solution-first thinking, vague metrics, etc.)

LLMs must not:
- Allow solution-first PRDs that skip problem definition
- Accept vague success metrics like "improve experience"
- Let users skip target audience definition
- Permit big-bang release plans without incremental milestones
- Fabricate information when users don't have answers
- Use Markdown instead of Confluence Wiki Markup
- Create overly detailed implementation specs (PRDs define requirements, not implementation)

---

These instructions should be used whenever an LLM assists with writing, reviewing, or improving PRDs, regardless of team or domain.
